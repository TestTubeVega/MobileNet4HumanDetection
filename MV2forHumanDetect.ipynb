{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTING HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration parameters\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 2  # Binary classification (human or no human)\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DATA_DIR = \"./human detection dataset\"  # Path to your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load human detection dataset from local directory and prepare for training.\"\"\"\n",
    "    print(\"Loading and preparing data from local directory...\")\n",
    "    \n",
    "    data_dir = pathlib.Path(DATA_DIR)\n",
    "    \n",
    "    # Verify the directory structure\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {DATA_DIR} not found!\")\n",
    "    \n",
    "    class_0_dir = data_dir / \"0\"  # No human\n",
    "    class_1_dir = data_dir / \"1\"  # Human present\n",
    "    \n",
    "    if not class_0_dir.exists() or not class_1_dir.exists():\n",
    "        raise FileNotFoundError(\"Expected subdirectories '0' and '1' not found!\")\n",
    "    \n",
    "    # Count files in each class\n",
    "    class_0_files = list(class_0_dir.glob('*'))\n",
    "    class_1_files = list(class_1_dir.glob('*'))\n",
    "    \n",
    "    print(f\"Found {len(class_0_files)} images without humans\")\n",
    "    print(f\"Found {len(class_1_files)} images with humans\")\n",
    "    \n",
    "    # Create image paths and labels lists\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Add class 0 (no human) images\n",
    "    for file_path in class_0_files:\n",
    "        if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            image_paths.append(str(file_path))\n",
    "            labels.append(0)\n",
    "    \n",
    "    # Add class 1 (human) images\n",
    "    for file_path in class_1_files:\n",
    "        if file_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "            image_paths.append(str(file_path))\n",
    "            labels.append(1)\n",
    "    \n",
    "    # Shuffle the data\n",
    "    temp = list(zip(image_paths, labels))\n",
    "    random.shuffle(temp)\n",
    "    image_paths, labels = zip(*temp)\n",
    "    \n",
    "    # Convert labels to numpy arrays\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    total_size = len(image_paths)\n",
    "    val_size = int(total_size * VALIDATION_SPLIT)\n",
    "    train_size = total_size - val_size\n",
    "    \n",
    "    train_paths = image_paths[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "    val_paths = image_paths[train_size:]\n",
    "    val_labels = labels[train_size:]\n",
    "    \n",
    "    print(f\"Training set: {len(train_paths)} images\")\n",
    "    print(f\"Validation set: {len(val_paths)} images\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_ds = create_dataset(train_paths, train_labels, is_training=True)\n",
    "    val_ds = create_dataset(val_paths, val_labels, is_training=False)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "def preprocess_image(image_path, label):\n",
    "    \"\"\"Load and preprocess a single image.\"\"\"\n",
    "    # Read image file\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    \n",
    "    # Convert image to float and normalize to [0, 1]\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    # Resize to target size\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    \n",
    "    # Ensure the image has 3 channels\n",
    "    img = tf.ensure_shape(img, [IMG_SIZE, IMG_SIZE, 3])\n",
    "    \n",
    "    # One-hot encode the label\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "def create_dataset(image_paths, labels, is_training=False):\n",
    "    \"\"\"Create a TensorFlow dataset from image paths and labels.\"\"\"\n",
    "    # Create dataset from paths and labels\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Apply data augmentation if training\n",
    "    if is_training:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layer\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MOBILENET V2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenetv2_model():\n",
    "    \"\"\"Build model using TensorFlow's pre-implemented MobileNetV2.\"\"\"\n",
    "    print(\"Building MobileNetV2 model for human detection...\")\n",
    "    \n",
    "    # Create the base model from the pre-trained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # First, freeze the base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create new model on top\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='MobileNetV2_HumanDetection')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "def unfreeze_model(model, base_model):\n",
    "    \"\"\"Unfreeze the base model for fine-tuning.\"\"\"\n",
    "    # Unfreeze the base model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze the first 100 layers (early layers)\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompile the model with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, val_ds, base_model):\n",
    "    \"\"\"Train the model in two phases: feature extraction and fine-tuning.\"\"\"\n",
    "    print(\"Training the model - Phase 1: Feature Extraction...\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Define callbacks for phase 1\n",
    "    callbacks_phase1 = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='mobilenetv2_human_detection_phase1.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Phase 1: Train with frozen base model (feature extraction)\n",
    "    history1 = model.fit(\n",
    "        train_ds,\n",
    "        epochs=5,  # Fewer epochs for initial phase\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks_phase1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining the model - Phase 2: Fine-tuning...\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning - unfreeze the model and train with a lower learning rate\n",
    "    model = unfreeze_model(model, base_model)\n",
    "    \n",
    "    # Define callbacks for phase 2\n",
    "    callbacks_phase2 = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='mobilenetv2_human_detection_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Phase 2: Train with unfrozen base model (fine-tuning)\n",
    "    history2 = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS-5,  # Remaining epochs for fine-tuning\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks_phase2\n",
    "    )\n",
    "    \n",
    "    # Combine histories\n",
    "    combined_history = {}\n",
    "    for k in history1.history.keys():\n",
    "        combined_history[k] = history1.history[k] + history2.history[k]\n",
    "    \n",
    "    return tf.keras.callbacks.History(), combined_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the training and validation accuracy and loss.\"\"\"\n",
    "    print(\"Plotting training history...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mobilenetv2_human_detection_training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_activations(model, val_ds):\n",
    "    \"\"\"Visualize intermediate activations to see what the model focuses on.\"\"\"\n",
    "    # Get an image from the validation set\n",
    "    for images, labels in val_ds.take(1):\n",
    "        sample_image = images[0]\n",
    "        sample_label = np.argmax(labels[0])\n",
    "        break\n",
    "    \n",
    "    # Create a modified model that outputs intermediate activations\n",
    "    layer_outputs = [layer.output for layer in model.layers[1].layers[50:60]]  # Get some intermediate layer outputs\n",
    "    activation_model = tf.keras.Model(inputs=model.layers[1].input, outputs=layer_outputs)\n",
    "    \n",
    "    # Get activations\n",
    "    activations = activation_model(tf.expand_dims(sample_image, 0))\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(3, 4, 1)\n",
    "    \n",
    "    # Convert from preprocessing format back to display format\n",
    "    display_image = sample_image.numpy()\n",
    "    display_image = ((display_image * 0.5) + 0.5) * 255  # Reverse preprocessing\n",
    "    display_image = np.clip(display_image, 0, 255).astype('uint8')\n",
    "    \n",
    "    plt.imshow(display_image)\n",
    "    plt.title(f\"Original Image (Class: {'Human' if sample_label == 1 else 'No Human'})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display some activation maps\n",
    "    for i, activation in enumerate(activations[:9]):\n",
    "        plt.subplot(3, 4, i+2)\n",
    "        plt.imshow(activation[0, :, :, 0], cmap='viridis')\n",
    "        plt.title(f\"Activation {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mobilenetv2_human_detection_activations.png')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, val_ds):\n",
    "    \"\"\"Evaluate the model on validation data.\"\"\"\n",
    "    print(\"Evaluating the model...\")\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    val_loss, val_accuracy = model.evaluate(val_ds)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Get a batch of validation data\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Collect predictions on the validation set\n",
    "    for images, labels in val_ds:\n",
    "        predictions = model.predict(images)\n",
    "        all_images.extend(images.numpy())\n",
    "        all_labels.extend(np.argmax(labels.numpy(), axis=1))\n",
    "        all_predictions.extend(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_images = np.array(all_images)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Plot some predictions\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(9, len(all_images))):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        \n",
    "        # Convert from preprocessing format back to display format\n",
    "        display_image = all_images[i]\n",
    "        display_image = ((display_image * 0.5) + 0.5) * 255  # Reverse preprocessing\n",
    "        display_image = np.clip(display_image, 0, 255).astype('uint8')\n",
    "        \n",
    "        plt.imshow(display_image)\n",
    "        color = 'green' if all_predictions[i] == all_labels[i] else 'red'\n",
    "        title = f\"Pred: {'Human' if all_predictions[i] == 1 else 'No Human'}\\nTrue: {'Human' if all_labels[i] == 1 else 'No Human'}\"\n",
    "        plt.title(title, color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mobilenetv2_human_detection_predictions.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(NUM_CLASSES)\n",
    "    plt.xticks(tick_marks, ['No Human', 'Human'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['No Human', 'Human'])\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('mobilenetv2_human_detection_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=['No Human', 'Human']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_deployment(model):\n",
    "    \"\"\"Save the model for deployment.\"\"\"\n",
    "    print(\"Saving model for deployment...\")\n",
    "    \n",
    "    # Save the entire model\n",
    "    model.save('mobilenetv2_human_detection_model')\n",
    "    \n",
    "    # Save as TensorFlow Lite model\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open('mobilenetv2_human_detection_model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Create TFLite model with quantization for smaller size\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    tflite_fp16_model = converter.convert()\n",
    "    \n",
    "    with open('mobilenetv2_human_detection_model_quantized.tflite', 'wb') as f:\n",
    "        f.write(tflite_fp16_model)\n",
    "    \n",
    "    print(\"Model saved for deployment.\")\n",
    "\n",
    "def create_inference_function(model):\n",
    "    \"\"\"Create a function for easy inference on new images.\"\"\"\n",
    "    def predict_human_in_image(image_path):\n",
    "        # Read image file\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        \n",
    "        # Apply MobileNetV2 preprocessing\n",
    "        img = preprocess_input(img)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img)\n",
    "        predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "        probability = prediction[0][predicted_class] * 100\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            'prediction': 'Human' if predicted_class == 1 else 'No Human',\n",
    "            'probability': float(probability),\n",
    "            'is_human': bool(predicted_class == 1)\n",
    "        }\n",
    "        \n",
    "        # Display the image with prediction\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        \n",
    "        # Convert from preprocessing format back to display format\n",
    "        display_img = img[0].numpy()\n",
    "        display_img = ((display_img * 0.5) + 0.5) * 255  # Reverse preprocessing\n",
    "        display_img = np.clip(display_img, 0, 255).astype('uint8')\n",
    "        \n",
    "        plt.imshow(display_img)\n",
    "        plt.title(f\"Prediction: {result['prediction']} ({result['probability']:.2f}%)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return predict_human_in_image\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete pipeline.\"\"\"\n",
    "    # Check for GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"Available GPUs: {len(gpus)}\")\n",
    "        # Limit memory growth to avoid OOM errors\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        print(\"No GPU found, running on CPU\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    train_ds, val_ds = load_and_prepare_data()\n",
    "    \n",
    "    # Build model\n",
    "    model, base_model = build_mobilenetv2_model()\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history_obj, history_dict = train_model(model, train_ds, val_ds, base_model)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history_dict)\n",
    "    \n",
    "    # Visualize activations\n",
    "    visualize_activations(model, val_ds)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model(model, val_ds)\n",
    "    \n",
    "    # Save model for deployment\n",
    "    save_model_for_deployment(model)\n",
    "    \n",
    "    # Create and test inference function\n",
    "    predict_human = create_inference_function(model)\n",
    "    \n",
    "    print(\"\\nModel training and evaluation complete!\")\n",
    "    print(\"You can now use the model for inference on new images.\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"predict_human('path/to/your/image.jpg')\")\n",
    "    \n",
    "    return predict_human\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_human = main()\n",
    "    \n",
    "    # Example of how to use the inference function (uncomment to test)\n",
    "    # test_image = \"./path/to/test/image.jpg\"\n",
    "    # if os.path.exists(test_image):\n",
    "    #     result = predict_human(test_image)\n",
    "    #     print(f\"\\nTest prediction on {test_image}:\")\n",
    "    #     print(f\"Prediction: {result['prediction']}\")\n",
    "    #     print(f\"Probability: {result['probability']:.2f}%\")\n",
    "    #     print(f\"Is human present: {result['is_human']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
