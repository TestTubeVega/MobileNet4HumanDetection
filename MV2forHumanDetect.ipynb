{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 15:03:15.674448: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-04-26 15:03:15.674513: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTING HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration parameters\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 2  # Binary classification (human or no human)\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DATA_DIR = \"./human detection dataset\"  # Path to your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 362 images without humans\n",
      "Found 559 images with humans\n",
      "Training set: 737 images\n",
      "Validation set: 184 images\n",
      "('./human detection dataset/1/310.png', './human detection dataset/1/240.png', './human detection dataset/1/167.png', './human detection dataset/1/260.png', './human detection dataset/1/356.png', './human detection dataset/1/151.png', './human detection dataset/1/262.png', './human detection dataset/1/177.png', './human detection dataset/0/224.png', './human detection dataset/0/49.png', './human detection dataset/1/536.png', './human detection dataset/1/352.png', './human detection dataset/1/542.png', './human detection dataset/0/290.png', './human detection dataset/0/65.png', './human detection dataset/1/385.png', './human detection dataset/1/76.png', './human detection dataset/0/98.png', './human detection dataset/0/72.png', './human detection dataset/1/538.png', './human detection dataset/1/215.png', './human detection dataset/1/443.png', './human detection dataset/1/312.png', './human detection dataset/0/346.png', './human detection dataset/1/264.png', './human detection dataset/1/32.png', './human detection dataset/1/359.png', './human detection dataset/1/28.png', './human detection dataset/0/331.png', './human detection dataset/1/90.png', './human detection dataset/1/418.png', './human detection dataset/0/355.png', './human detection dataset/0/337.png', './human detection dataset/1/291.png', './human detection dataset/0/218.png', './human detection dataset/1/364.png', './human detection dataset/0/82.png', './human detection dataset/0/73.png', './human detection dataset/0/173.png', './human detection dataset/1/499.png', './human detection dataset/0/259.png', './human detection dataset/0/237.png', './human detection dataset/0/227.png', './human detection dataset/0/231.png', './human detection dataset/1/214.png', './human detection dataset/0/177.png', './human detection dataset/0/117.png', './human detection dataset/1/416.png', './human detection dataset/0/255.png', './human detection dataset/1/6.png', './human detection dataset/1/239.png', './human detection dataset/0/198.png', './human detection dataset/1/476.png', './human detection dataset/1/504.png', './human detection dataset/0/27.png', './human detection dataset/0/8.png', './human detection dataset/1/100.png', './human detection dataset/1/471.png', './human detection dataset/0/53.png', './human detection dataset/1/148.png', './human detection dataset/0/115.png', './human detection dataset/1/369.png', './human detection dataset/1/419.png', './human detection dataset/0/202.png', './human detection dataset/0/165.png', './human detection dataset/1/2.png', './human detection dataset/0/303.png', './human detection dataset/1/452.png', './human detection dataset/0/28.png', './human detection dataset/0/211.png', './human detection dataset/1/548.png', './human detection dataset/0/25.png', './human detection dataset/1/33.png', './human detection dataset/0/326.png', './human detection dataset/0/251.png', './human detection dataset/0/230.png', './human detection dataset/0/242.png', './human detection dataset/1/362.png', './human detection dataset/0/212.png', './human detection dataset/1/217.png', './human detection dataset/0/71.png', './human detection dataset/0/79.png', './human detection dataset/1/247.png', './human detection dataset/0/340.png', './human detection dataset/0/147.png', './human detection dataset/1/192.png', './human detection dataset/1/372.png', './human detection dataset/1/417.png', './human detection dataset/0/167.png', './human detection dataset/1/554.png', './human detection dataset/0/275.png', './human detection dataset/1/282.png', './human detection dataset/1/248.png', './human detection dataset/1/322.png', './human detection dataset/1/533.png', './human detection dataset/1/114.png', './human detection dataset/1/231.png', './human detection dataset/1/120.png', './human detection dataset/1/519.png', './human detection dataset/0/318.png', './human detection dataset/1/127.png', './human detection dataset/0/96.png', './human detection dataset/0/332.png', './human detection dataset/0/12.png', './human detection dataset/1/329.png', './human detection dataset/1/351.png', './human detection dataset/1/13.png', './human detection dataset/1/181.png', './human detection dataset/0/250.png', './human detection dataset/1/271.png', './human detection dataset/1/492.png', './human detection dataset/1/424.png', './human detection dataset/1/556.png', './human detection dataset/0/75.png', './human detection dataset/1/275.png', './human detection dataset/1/11.png', './human detection dataset/1/552.png', './human detection dataset/0/221.png', './human detection dataset/0/24.png', './human detection dataset/1/484.png', './human detection dataset/0/313.png', './human detection dataset/1/508.png', './human detection dataset/1/27.png', './human detection dataset/1/407.png', './human detection dataset/1/535.png', './human detection dataset/1/491.png', './human detection dataset/1/525.png', './human detection dataset/0/154.png', './human detection dataset/1/450.png', './human detection dataset/1/125.png', './human detection dataset/0/285.png', './human detection dataset/1/198.png', './human detection dataset/1/458.png', './human detection dataset/1/502.png', './human detection dataset/0/168.png', './human detection dataset/1/196.png', './human detection dataset/1/439.png', './human detection dataset/0/38.png', './human detection dataset/1/249.png', './human detection dataset/0/268.png', './human detection dataset/1/557.png', './human detection dataset/1/370.png', './human detection dataset/1/464.png', './human detection dataset/1/480.png', './human detection dataset/0/276.png', './human detection dataset/1/426.png', './human detection dataset/0/162.png', './human detection dataset/1/401.png', './human detection dataset/0/234.png', './human detection dataset/1/384.png', './human detection dataset/0/200.png', './human detection dataset/1/487.png', './human detection dataset/1/121.png', './human detection dataset/0/350.png', './human detection dataset/1/243.png', './human detection dataset/0/286.png', './human detection dataset/1/8.png', './human detection dataset/0/19.png', './human detection dataset/0/105.png', './human detection dataset/0/267.png', './human detection dataset/1/290.png', './human detection dataset/0/345.png', './human detection dataset/1/225.png', './human detection dataset/1/50.png', './human detection dataset/1/339.png', './human detection dataset/0/128.png', './human detection dataset/0/191.png', './human detection dataset/1/252.png', './human detection dataset/0/297.png', './human detection dataset/1/387.png', './human detection dataset/1/70.png', './human detection dataset/1/431.png', './human detection dataset/1/501.png', './human detection dataset/1/474.png', './human detection dataset/1/402.png', './human detection dataset/0/112.png', './human detection dataset/1/376.png', './human detection dataset/0/120.png', './human detection dataset/1/327.png', './human detection dataset/1/333.png', './human detection dataset/0/357.png', './human detection dataset/0/292.png', './human detection dataset/1/462.png', './human detection dataset/1/457.png', './human detection dataset/1/65.png', './human detection dataset/0/185.png', './human detection dataset/1/383.png', './human detection dataset/1/115.png', './human detection dataset/0/40.png', './human detection dataset/1/360.png', './human detection dataset/0/57.png', './human detection dataset/1/226.png', './human detection dataset/0/348.png', './human detection dataset/1/354.png', './human detection dataset/1/318.png', './human detection dataset/1/498.png', './human detection dataset/1/64.png', './human detection dataset/1/183.png', './human detection dataset/1/433.png', './human detection dataset/1/306.png', './human detection dataset/1/497.png', './human detection dataset/0/194.png', './human detection dataset/0/233.png', './human detection dataset/1/136.png', './human detection dataset/0/196.png', './human detection dataset/1/165.png', './human detection dataset/0/70.png', './human detection dataset/0/26.png', './human detection dataset/0/135.png', './human detection dataset/1/379.png', './human detection dataset/0/122.png', './human detection dataset/1/405.png', './human detection dataset/1/549.png', './human detection dataset/1/92.png', './human detection dataset/0/156.png', './human detection dataset/1/473.png', './human detection dataset/1/230.png', './human detection dataset/1/304.png', './human detection dataset/1/63.png', './human detection dataset/0/192.png', './human detection dataset/0/36.png', './human detection dataset/1/551.png', './human detection dataset/1/102.png', './human detection dataset/1/227.png', './human detection dataset/0/11.png', './human detection dataset/1/390.png', './human detection dataset/1/350.png', './human detection dataset/1/313.png', './human detection dataset/1/399.png', './human detection dataset/1/486.png', './human detection dataset/0/264.png', './human detection dataset/0/78.png', './human detection dataset/0/183.png', './human detection dataset/0/201.png', './human detection dataset/1/18.png', './human detection dataset/1/456.png', './human detection dataset/0/243.png', './human detection dataset/0/205.png', './human detection dataset/0/129.png', './human detection dataset/0/76.png', './human detection dataset/1/85.png', './human detection dataset/1/301.png', './human detection dataset/0/108.png', './human detection dataset/1/124.png', './human detection dataset/1/40.png', './human detection dataset/1/23.png', './human detection dataset/1/67.png', './human detection dataset/1/406.png', './human detection dataset/1/158.png', './human detection dataset/1/26.png', './human detection dataset/0/284.png', './human detection dataset/0/33.png', './human detection dataset/1/173.png', './human detection dataset/1/315.png', './human detection dataset/0/353.png', './human detection dataset/0/47.png', './human detection dataset/0/356.png', './human detection dataset/1/389.png', './human detection dataset/1/303.png', './human detection dataset/1/160.png', './human detection dataset/1/349.png', './human detection dataset/1/197.png', './human detection dataset/1/540.png', './human detection dataset/0/125.png', './human detection dataset/1/516.png', './human detection dataset/1/14.png', './human detection dataset/0/184.png', './human detection dataset/1/340.png', './human detection dataset/1/365.png', './human detection dataset/0/244.png', './human detection dataset/0/160.png', './human detection dataset/0/354.png', './human detection dataset/1/164.png', './human detection dataset/1/295.png', './human detection dataset/1/269.png', './human detection dataset/0/302.png', './human detection dataset/1/461.png', './human detection dataset/0/182.png', './human detection dataset/1/463.png', './human detection dataset/0/17.png', './human detection dataset/1/37.png', './human detection dataset/1/129.png', './human detection dataset/0/319.png', './human detection dataset/1/311.png', './human detection dataset/1/378.png', './human detection dataset/0/161.png', './human detection dataset/1/341.png', './human detection dataset/1/142.png', './human detection dataset/1/80.png', './human detection dataset/1/544.png', './human detection dataset/0/257.png', './human detection dataset/1/345.png', './human detection dataset/1/528.png', './human detection dataset/1/460.png', './human detection dataset/1/288.png', './human detection dataset/0/64.png', './human detection dataset/1/219.png', './human detection dataset/1/221.png', './human detection dataset/1/468.png', './human detection dataset/1/437.png', './human detection dataset/1/22.png', './human detection dataset/1/206.png', './human detection dataset/1/49.png', './human detection dataset/1/280.png', './human detection dataset/0/31.png', './human detection dataset/0/157.png', './human detection dataset/1/420.png', './human detection dataset/1/481.png', './human detection dataset/1/185.png', './human detection dataset/1/409.png', './human detection dataset/1/348.png', './human detection dataset/0/240.png', './human detection dataset/1/56.png', './human detection dataset/0/258.png', './human detection dataset/0/6.png', './human detection dataset/1/96.png', './human detection dataset/0/265.png', './human detection dataset/1/298.png', './human detection dataset/0/287.png', './human detection dataset/1/328.png', './human detection dataset/1/470.png', './human detection dataset/0/226.png', './human detection dataset/0/247.png', './human detection dataset/0/15.png', './human detection dataset/1/530.png', './human detection dataset/0/266.png', './human detection dataset/1/0.png', './human detection dataset/1/178.png', './human detection dataset/0/54.png', './human detection dataset/1/325.png', './human detection dataset/1/539.png', './human detection dataset/0/142.png', './human detection dataset/1/189.png', './human detection dataset/1/94.png', './human detection dataset/1/133.png', './human detection dataset/1/187.png', './human detection dataset/1/246.png', './human detection dataset/1/35.png', './human detection dataset/0/118.png', './human detection dataset/1/320.png', './human detection dataset/0/296.png', './human detection dataset/1/430.png', './human detection dataset/0/300.png', './human detection dataset/1/337.png', './human detection dataset/0/349.png', './human detection dataset/1/207.png', './human detection dataset/0/339.png', './human detection dataset/0/273.png', './human detection dataset/0/95.png', './human detection dataset/1/144.png', './human detection dataset/0/334.png', './human detection dataset/0/351.png', './human detection dataset/1/134.png', './human detection dataset/1/277.png', './human detection dataset/1/308.png', './human detection dataset/0/320.png', './human detection dataset/1/506.png', './human detection dataset/0/241.png', './human detection dataset/1/71.png', './human detection dataset/1/381.png', './human detection dataset/1/186.png', './human detection dataset/0/150.png', './human detection dataset/1/267.png', './human detection dataset/0/270.png', './human detection dataset/1/545.png', './human detection dataset/1/191.png', './human detection dataset/1/255.png', './human detection dataset/0/4.png', './human detection dataset/0/37.png', './human detection dataset/0/3.png', './human detection dataset/1/394.png', './human detection dataset/1/475.png', './human detection dataset/1/150.png', './human detection dataset/0/293.png', './human detection dataset/1/111.png', './human detection dataset/1/527.png', './human detection dataset/0/181.png', './human detection dataset/1/171.png', './human detection dataset/0/145.png', './human detection dataset/0/110.png', './human detection dataset/0/133.png', './human detection dataset/0/29.png', './human detection dataset/0/116.png', './human detection dataset/0/169.png', './human detection dataset/1/199.png', './human detection dataset/1/98.png', './human detection dataset/0/93.png', './human detection dataset/1/130.png', './human detection dataset/0/80.png', './human detection dataset/1/316.png', './human detection dataset/1/238.png', './human detection dataset/1/223.png', './human detection dataset/1/478.png', './human detection dataset/1/180.png', './human detection dataset/0/209.png', './human detection dataset/0/41.png', './human detection dataset/1/289.png', './human detection dataset/1/101.png', './human detection dataset/1/274.png', './human detection dataset/1/237.png', './human detection dataset/1/146.png', './human detection dataset/1/521.png', './human detection dataset/0/104.png', './human detection dataset/0/309.png', './human detection dataset/0/69.png', './human detection dataset/1/202.png', './human detection dataset/1/515.png', './human detection dataset/1/131.png', './human detection dataset/1/117.png', './human detection dataset/1/48.png', './human detection dataset/1/228.png', './human detection dataset/1/343.png', './human detection dataset/0/136.png', './human detection dataset/0/288.png', './human detection dataset/1/534.png', './human detection dataset/1/330.png', './human detection dataset/1/209.png', './human detection dataset/0/216.png', './human detection dataset/1/429.png', './human detection dataset/0/179.png', './human detection dataset/0/274.png', './human detection dataset/1/244.png', './human detection dataset/1/88.png', './human detection dataset/0/83.png', './human detection dataset/1/161.png', './human detection dataset/1/205.png', './human detection dataset/1/357.png', './human detection dataset/1/229.png', './human detection dataset/1/466.png', './human detection dataset/1/382.png', './human detection dataset/1/451.png', './human detection dataset/0/220.png', './human detection dataset/1/213.png', './human detection dataset/1/45.png', './human detection dataset/0/301.png', './human detection dataset/0/124.png', './human detection dataset/1/166.png', './human detection dataset/0/308.png', './human detection dataset/1/245.png', './human detection dataset/1/393.png', './human detection dataset/1/375.png', './human detection dataset/1/172.png', './human detection dataset/0/316.png', './human detection dataset/1/294.png', './human detection dataset/0/137.png', './human detection dataset/0/101.png', './human detection dataset/0/46.png', './human detection dataset/1/195.png', './human detection dataset/0/283.png', './human detection dataset/1/529.png', './human detection dataset/0/306.png', './human detection dataset/0/333.png', './human detection dataset/1/110.png', './human detection dataset/0/55.png', './human detection dataset/0/89.png', './human detection dataset/0/343.png', './human detection dataset/1/254.png', './human detection dataset/1/500.png', './human detection dataset/1/220.png', './human detection dataset/1/293.png', './human detection dataset/1/469.png', './human detection dataset/1/366.png', './human detection dataset/1/522.png', './human detection dataset/0/22.png', './human detection dataset/1/105.png', './human detection dataset/1/31.png', './human detection dataset/1/334.png', './human detection dataset/0/281.png', './human detection dataset/0/219.png', './human detection dataset/0/330.png', './human detection dataset/0/77.png', './human detection dataset/0/352.png', './human detection dataset/1/388.png', './human detection dataset/0/307.png', './human detection dataset/1/176.png', './human detection dataset/0/56.png', './human detection dataset/1/72.png', './human detection dataset/1/259.png', './human detection dataset/1/3.png', './human detection dataset/1/16.png', './human detection dataset/0/210.png', './human detection dataset/1/170.png', './human detection dataset/0/199.png', './human detection dataset/1/317.png', './human detection dataset/0/87.png', './human detection dataset/1/132.png', './human detection dataset/1/203.png', './human detection dataset/1/483.png', './human detection dataset/0/299.png', './human detection dataset/1/555.png', './human detection dataset/0/203.png', './human detection dataset/1/235.png', './human detection dataset/1/494.png', './human detection dataset/1/485.png', './human detection dataset/1/408.png', './human detection dataset/0/314.png', './human detection dataset/1/204.png', './human detection dataset/0/253.png', './human detection dataset/1/57.png', './human detection dataset/1/281.png', './human detection dataset/1/188.png', './human detection dataset/0/9.png', './human detection dataset/1/84.png', './human detection dataset/1/168.png', './human detection dataset/1/89.png', './human detection dataset/1/256.png', './human detection dataset/1/374.png', './human detection dataset/1/91.png', './human detection dataset/1/319.png', './human detection dataset/0/159.png', './human detection dataset/1/242.png', './human detection dataset/1/87.png', './human detection dataset/0/341.png', './human detection dataset/0/94.png', './human detection dataset/0/43.png', './human detection dataset/1/153.png', './human detection dataset/1/66.png', './human detection dataset/1/77.png', './human detection dataset/0/282.png', './human detection dataset/0/138.png', './human detection dataset/0/305.png', './human detection dataset/1/455.png', './human detection dataset/0/289.png', './human detection dataset/1/440.png', './human detection dataset/0/208.png', './human detection dataset/1/272.png', './human detection dataset/1/104.png', './human detection dataset/1/53.png', './human detection dataset/0/60.png', './human detection dataset/1/373.png', './human detection dataset/1/509.png', './human detection dataset/0/232.png', './human detection dataset/1/17.png', './human detection dataset/1/386.png', './human detection dataset/0/321.png', './human detection dataset/1/299.png', './human detection dataset/1/113.png', './human detection dataset/1/38.png', './human detection dataset/0/359.png', './human detection dataset/0/215.png', './human detection dataset/0/186.png', './human detection dataset/1/116.png', './human detection dataset/1/512.png', './human detection dataset/1/335.png', './human detection dataset/1/392.png', './human detection dataset/1/398.png', './human detection dataset/0/193.png', './human detection dataset/1/51.png', './human detection dataset/0/235.png', './human detection dataset/0/213.png', './human detection dataset/1/338.png', './human detection dataset/1/145.png', './human detection dataset/0/323.png', './human detection dataset/1/326.png', './human detection dataset/1/447.png', './human detection dataset/1/46.png', './human detection dataset/0/74.png', './human detection dataset/0/148.png', './human detection dataset/1/284.png', './human detection dataset/1/493.png', './human detection dataset/1/283.png', './human detection dataset/1/266.png', './human detection dataset/1/61.png', './human detection dataset/0/279.png', './human detection dataset/1/444.png', './human detection dataset/0/134.png', './human detection dataset/0/61.png', './human detection dataset/1/97.png', './human detection dataset/0/63.png', './human detection dataset/0/187.png', './human detection dataset/0/152.png', './human detection dataset/1/517.png', './human detection dataset/1/543.png', './human detection dataset/1/532.png', './human detection dataset/0/238.png', './human detection dataset/1/403.png', './human detection dataset/0/111.png', './human detection dataset/1/553.png', './human detection dataset/0/7.png', './human detection dataset/0/310.png', './human detection dataset/0/158.png', './human detection dataset/1/175.png', './human detection dataset/0/204.png', './human detection dataset/0/59.png', './human detection dataset/1/81.png', './human detection dataset/1/39.png', './human detection dataset/0/361.png', './human detection dataset/1/233.png', './human detection dataset/0/294.png', './human detection dataset/1/190.png', './human detection dataset/1/135.png', './human detection dataset/1/411.png', './human detection dataset/1/200.png', './human detection dataset/1/54.png', './human detection dataset/1/273.png', './human detection dataset/1/300.png', './human detection dataset/1/438.png', './human detection dataset/0/254.png', './human detection dataset/0/151.png', './human detection dataset/1/261.png', './human detection dataset/1/355.png', './human detection dataset/0/52.png', './human detection dataset/0/172.png', './human detection dataset/1/156.png', './human detection dataset/1/108.png', './human detection dataset/0/163.png', './human detection dataset/0/92.png', './human detection dataset/1/367.png', './human detection dataset/0/21.png', './human detection dataset/0/298.png', './human detection dataset/1/514.png', './human detection dataset/1/434.png', './human detection dataset/1/459.png', './human detection dataset/0/48.png', './human detection dataset/0/217.png', './human detection dataset/1/477.png', './human detection dataset/0/123.png', './human detection dataset/0/35.png', './human detection dataset/0/236.png', './human detection dataset/0/103.png', './human detection dataset/1/78.png', './human detection dataset/0/23.png', './human detection dataset/0/207.png', './human detection dataset/0/256.png', './human detection dataset/0/262.png', './human detection dataset/0/329.png', './human detection dataset/0/328.png', './human detection dataset/1/193.png', './human detection dataset/0/252.png', './human detection dataset/0/66.png', './human detection dataset/0/263.png', './human detection dataset/1/503.png', './human detection dataset/0/206.png', './human detection dataset/1/73.png', './human detection dataset/0/100.png', './human detection dataset/1/495.png', './human detection dataset/1/216.png', './human detection dataset/1/147.png', './human detection dataset/1/19.png', './human detection dataset/1/236.png', './human detection dataset/1/12.png', './human detection dataset/0/324.png', './human detection dataset/1/448.png', './human detection dataset/1/285.png', './human detection dataset/1/169.png', './human detection dataset/0/130.png', './human detection dataset/1/321.png', './human detection dataset/1/324.png', './human detection dataset/0/30.png', './human detection dataset/1/412.png', './human detection dataset/1/9.png', './human detection dataset/0/174.png', './human detection dataset/0/325.png', './human detection dataset/1/489.png', './human detection dataset/0/10.png', './human detection dataset/1/453.png', './human detection dataset/0/336.png', './human detection dataset/1/526.png', './human detection dataset/1/4.png', './human detection dataset/1/103.png', './human detection dataset/0/171.png', './human detection dataset/1/152.png', './human detection dataset/1/278.png', './human detection dataset/0/245.png', './human detection dataset/0/91.png', './human detection dataset/0/16.png', './human detection dataset/1/467.png', './human detection dataset/1/344.png', './human detection dataset/0/140.png', './human detection dataset/0/113.png', './human detection dataset/1/270.png', './human detection dataset/0/317.png', './human detection dataset/0/248.png', './human detection dataset/1/368.png', './human detection dataset/1/174.png', './human detection dataset/0/127.png', './human detection dataset/1/107.png', './human detection dataset/1/347.png', './human detection dataset/0/197.png', './human detection dataset/1/454.png', './human detection dataset/1/118.png', './human detection dataset/0/344.png', './human detection dataset/1/210.png', './human detection dataset/1/42.png', './human detection dataset/1/25.png', './human detection dataset/1/258.png', './human detection dataset/1/558.png', './human detection dataset/0/260.png', './human detection dataset/1/232.png', './human detection dataset/1/537.png', './human detection dataset/1/201.png', './human detection dataset/1/222.png', './human detection dataset/0/86.png', './human detection dataset/1/307.png', './human detection dataset/1/137.png', './human detection dataset/1/43.png', './human detection dataset/1/250.png', './human detection dataset/0/223.png', './human detection dataset/0/315.png', './human detection dataset/1/218.png', './human detection dataset/1/371.png', './human detection dataset/0/175.png', './human detection dataset/1/241.png', './human detection dataset/1/465.png', './human detection dataset/1/99.png', './human detection dataset/0/190.png', './human detection dataset/1/155.png', './human detection dataset/0/195.png', './human detection dataset/0/106.png', './human detection dataset/1/163.png', './human detection dataset/0/249.png', './human detection dataset/1/518.png', './human detection dataset/1/302.png', './human detection dataset/1/380.png', './human detection dataset/1/524.png', './human detection dataset/1/69.png', './human detection dataset/0/229.png', './human detection dataset/0/50.png', './human detection dataset/0/149.png', './human detection dataset/0/261.png', './human detection dataset/0/68.png', './human detection dataset/1/75.png', './human detection dataset/1/436.png', './human detection dataset/1/194.png', './human detection dataset/1/395.png', './human detection dataset/1/74.png', './human detection dataset/0/180.png', './human detection dataset/1/427.png', './human detection dataset/0/62.png', './human detection dataset/1/21.png', './human detection dataset/0/131.png', './human detection dataset/1/286.png', './human detection dataset/1/154.png', './human detection dataset/1/41.png', './human detection dataset/1/361.png', './human detection dataset/1/496.png', './human detection dataset/0/42.png')\n",
      "[1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1\n",
      " 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 1 0 1\n",
      " 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1\n",
      " 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1\n",
      " 1 1 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
      " 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1\n",
      " 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0\n",
      " 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1\n",
      " 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0\n",
      " 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
      " 1 0 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 1\n",
      " 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unbatching a tensor is only supported for rank >= 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_labels)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Create TensorFlow datasets\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m create_dataset(val_paths, val_labels, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(image_paths, labels, is_training)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a TensorFlow dataset from image paths and labels.\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create dataset from paths and labels\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Load and preprocess images\u001b[39;00m\n\u001b[1;32m     34\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(preprocess_image, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n",
      "File \u001b[0;32m~/miniconda3/envs/raspi/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:781\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 781\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/raspi/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4666\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors:\n\u001b[1;32m   4665\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `element`. `element` should not be empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4667\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched_spec\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata \u001b[38;5;241m=\u001b[39m dataset_metadata_pb2\u001b[38;5;241m.\u001b[39mMetadata()\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n",
      "File \u001b[0;32m~/miniconda3/envs/raspi/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:234\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m    230\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    231\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 234\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries])\n",
      "File \u001b[0;32m~/miniconda3/envs/raspi/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:234\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    230\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    231\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 234\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries])\n",
      "File \u001b[0;32m~/miniconda3/envs/raspi/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4667\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__.<locals>.<lambda>\u001b[0;34m(component_spec)\u001b[0m\n\u001b[1;32m   4664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors:\n\u001b[1;32m   4665\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `element`. `element` should not be empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m-> 4667\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m component_spec: \u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, batched_spec)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata \u001b[38;5;241m=\u001b[39m dataset_metadata_pb2\u001b[38;5;241m.\u001b[39mMetadata()\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name:\n",
      "File \u001b[0;32m~/miniconda3/envs/raspi/lib/python3.9/site-packages/tensorflow/python/framework/tensor_spec.py:220\u001b[0m, in \u001b[0;36mTensorSpec._unbatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_unbatch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape\u001b[38;5;241m.\u001b[39mndims \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnbatching a tensor is only supported for rank >= 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    221\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m TensorSpec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Unbatching a tensor is only supported for rank >= 1"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path, label):\n",
    "    \"\"\"Load and preprocess a single image.\"\"\"\n",
    "    # Read image file\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "    \n",
    "    # Convert image to float and normalize to [0, 1]\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    \n",
    "    # Resize to target size\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    \n",
    "    # Ensure the image has 3 channels\n",
    "    img = tf.ensure_shape(img, [IMG_SIZE, IMG_SIZE, 3])\n",
    "    \n",
    "    # One-hot encode the label\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "def create_dataset(image_paths, labels, is_training=False):\n",
    "    \"\"\"Create a TensorFlow dataset from image paths and labels.\"\"\"\n",
    "    # Create dataset from paths and labels\n",
    "    ds = tf.data.Dataset.from_tensor_slices(image_paths, labels)\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Apply data augmentation if training\n",
    "    if is_training:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "class_0_dir = os.path.join(DATA_DIR, \"0\" )\n",
    "class_1_dir = os.path.join(DATA_DIR, \"1\" ) \n",
    "\n",
    "# Count files in each class\n",
    "class_0_files = os.listdir(class_0_dir)\n",
    "class_1_files = os.listdir(class_1_dir)\n",
    "\n",
    "print(f\"Found {len(class_0_files)} images without humans\")\n",
    "print(f\"Found {len(class_1_files)} images with humans\")\n",
    "\n",
    "# Create image paths and labels lists\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "# Add class 0 (no human) images\n",
    "for file_path in class_0_files:\n",
    "    image_paths.append(str(os.path.join(DATA_DIR, \"0\", file_path)))\n",
    "    labels.append(0)\n",
    "\n",
    "# Add class 1 (human) images\n",
    "for file_path in class_1_files:\n",
    "    image_paths.append(str(os.path.join(DATA_DIR, \"1\", file_path)))\n",
    "    labels.append(1)\n",
    "\n",
    "# Shuffle the data\n",
    "temp = list(zip(image_paths, labels))\n",
    "random.shuffle(temp)\n",
    "image_paths, labels = zip(*temp)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split into train and validation sets\n",
    "total_size = len(image_paths)\n",
    "val_size = int(total_size * VALIDATION_SPLIT)\n",
    "train_size = total_size - val_size\n",
    "\n",
    "train_paths = image_paths[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "val_paths = image_paths[train_size:]\n",
    "val_labels = labels[train_size:]\n",
    "\n",
    "print(f\"Training set: {len(train_paths)} images\")\n",
    "print(f\"Validation set: {len(val_paths)} images\")\n",
    "\n",
    "print(train_paths)\n",
    "print(train_labels)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_ds = create_dataset(train_paths, train_labels, is_training=True)\n",
    "val_ds = create_dataset(val_paths, val_labels, is_training=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD MOBILENET V2 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenetv2_model():\n",
    "    \"\"\"Build model using TensorFlow's pre-implemented MobileNetV2.\"\"\"\n",
    "    print(\"Building MobileNetV2 model for human detection...\")\n",
    "    \n",
    "    # Create the base model from the pre-trained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # First, freeze the base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Create new model on top\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='MobileNetV2_HumanDetection')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "def unfreeze_model(model, base_model):\n",
    "    \"\"\"Unfreeze the base model for fine-tuning.\"\"\"\n",
    "    # Unfreeze the base model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze the first 100 layers (early layers)\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompile the model with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, val_ds, base_model):\n",
    "    \"\"\"Train the model in two phases: feature extraction and fine-tuning.\"\"\"\n",
    "    print(\"Training the model - Phase 1: Feature Extraction...\")\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Define callbacks for phase 1\n",
    "    callbacks_phase1 = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='mobilenetv2_human_detection_phase1.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Phase 1: Train with frozen base model (feature extraction)\n",
    "    history1 = model.fit(\n",
    "        train_ds,\n",
    "        epochs=5,  # Fewer epochs for initial phase\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks_phase1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining the model - Phase 2: Fine-tuning...\")\n",
    "    \n",
    "    # Phase 2: Fine-tuning - unfreeze the model and train with a lower learning rate\n",
    "    model = unfreeze_model(model, base_model)\n",
    "    \n",
    "    # Define callbacks for phase 2\n",
    "    callbacks_phase2 = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='mobilenetv2_human_detection_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Phase 2: Train with unfrozen base model (fine-tuning)\n",
    "    history2 = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS-5,  # Remaining epochs for fine-tuning\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks_phase2\n",
    "    )\n",
    "    \n",
    "    # Combine histories\n",
    "    combined_history = {}\n",
    "    for k in history1.history.keys():\n",
    "        combined_history[k] = history1.history[k] + history2.history[k]\n",
    "    \n",
    "    return tf.keras.callbacks.History(), combined_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the training and validation accuracy and loss.\"\"\"\n",
    "    print(\"Plotting training history...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mobilenetv2_human_detection_training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_activations(model, val_ds):\n",
    "    \"\"\"Visualize intermediate activations to see what the model focuses on.\"\"\"\n",
    "    # Get an image from the validation set\n",
    "    for images, labels in val_ds.take(1):\n",
    "        sample_image = images[0]\n",
    "        sample_label = np.argmax(labels[0])\n",
    "        break\n",
    "    \n",
    "    # Create a modified model that outputs intermediate activations\n",
    "    layer_outputs = [layer.output for layer in model.layers[1].layers[50:60]]  # Get some intermediate layer outputs\n",
    "    activation_model = tf.keras.Model(inputs=model.layers[1].input, outputs=layer_outputs)\n",
    "    \n",
    "    # Get activations\n",
    "    activations = activation_model(tf.expand_dims(sample_image, 0))\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(3, 4, 1)\n",
    "    \n",
    "    # Convert from preprocessing format back to display format\n",
    "    display_image = sample_image.numpy()\n",
    "    display_image = ((display_image * 0.5) + 0.5) * 255  # Reverse preprocessing\n",
    "    display_image = np.clip(display_image, 0, 255).astype('uint8')\n",
    "    \n",
    "    plt.imshow(display_image)\n",
    "    plt.title(f\"Original Image (Class: {'Human' if sample_label == 1 else 'No Human'})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Display some activation maps\n",
    "    for i, activation in enumerate(activations[:9]):\n",
    "        plt.subplot(3, 4, i+2)\n",
    "        plt.imshow(activation[0, :, :, 0], cmap='viridis')\n",
    "        plt.title(f\"Activation {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mobilenetv2_human_detection_activations.png')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, val_ds):\n",
    "    \"\"\"Evaluate the model on validation data.\"\"\"\n",
    "    print(\"Evaluating the model...\")\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    val_loss, val_accuracy = model.evaluate(val_ds)\n",
    "    print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Get a batch of validation data\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Collect predictions on the validation set\n",
    "    for images, labels in val_ds:\n",
    "        predictions = model.predict(images)\n",
    "        all_images.extend(images.numpy())\n",
    "        all_labels.extend(np.argmax(labels.numpy(), axis=1))\n",
    "        all_predictions.extend(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_images = np.array(all_images)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    \n",
    "    # Plot some predictions\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(min(9, len(all_images))):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        \n",
    "        # Convert from preprocessing format back to display format\n",
    "        display_image = all_images[i]\n",
    "        display_image = ((display_image * 0.5) + 0.5) * 255  # Reverse preprocessing\n",
    "        display_image = np.clip(display_image, 0, 255).astype('uint8')\n",
    "        \n",
    "        plt.imshow(display_image)\n",
    "        color = 'green' if all_predictions[i] == all_labels[i] else 'red'\n",
    "        title = f\"Pred: {'Human' if all_predictions[i] == 1 else 'No Human'}\\nTrue: {'Human' if all_labels[i] == 1 else 'No Human'}\"\n",
    "        plt.title(title, color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mobilenetv2_human_detection_predictions.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(NUM_CLASSES)\n",
    "    plt.xticks(tick_marks, ['No Human', 'Human'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['No Human', 'Human'])\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('mobilenetv2_human_detection_confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=['No Human', 'Human']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_for_deployment(model):\n",
    "    \"\"\"Save the model for deployment.\"\"\"\n",
    "    print(\"Saving model for deployment...\")\n",
    "    \n",
    "    # Save the entire model\n",
    "    model.save('mobilenetv2_human_detection_model')\n",
    "    \n",
    "    # Save as TensorFlow Lite model\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open('mobilenetv2_human_detection_model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Create TFLite model with quantization for smaller size\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    tflite_fp16_model = converter.convert()\n",
    "    \n",
    "    with open('mobilenetv2_human_detection_model_quantized.tflite', 'wb') as f:\n",
    "        f.write(tflite_fp16_model)\n",
    "    \n",
    "    print(\"Model saved for deployment.\")\n",
    "\n",
    "def create_inference_function(model):\n",
    "    \"\"\"Create a function for easy inference on new images.\"\"\"\n",
    "    def predict_human_in_image(image_path):\n",
    "        # Read image file\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        \n",
    "        # Apply MobileNetV2 preprocessing\n",
    "        img = preprocess_input(img)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img)\n",
    "        predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "        probability = prediction[0][predicted_class] * 100\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            'prediction': 'Human' if predicted_class == 1 else 'No Human',\n",
    "            'probability': float(probability),\n",
    "            'is_human': bool(predicted_class == 1)\n",
    "        }\n",
    "        \n",
    "        # Display the image with prediction\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        \n",
    "        # Convert from preprocessing format back to display format\n",
    "        display_img = img[0].numpy()\n",
    "        display_img = ((display_img * 0.5) + 0.5) * 255  # Reverse preprocessing\n",
    "        display_img = np.clip(display_img, 0, 255).astype('uint8')\n",
    "        \n",
    "        plt.imshow(display_img)\n",
    "        plt.title(f\"Prediction: {result['prediction']} ({result['probability']:.2f}%)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return predict_human_in_image\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete pipeline.\"\"\"\n",
    "    # Check for GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"Available GPUs: {len(gpus)}\")\n",
    "        # Limit memory growth to avoid OOM errors\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        print(\"No GPU found, running on CPU\")\n",
    "    \n",
    "    # Load and prepare data\n",
    "    train_ds, val_ds = load_and_prepare_data()\n",
    "    \n",
    "    # Build model\n",
    "    model, base_model = build_mobilenetv2_model()\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history_obj, history_dict = train_model(model, train_ds, val_ds, base_model)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history_dict)\n",
    "    \n",
    "    # Visualize activations\n",
    "    visualize_activations(model, val_ds)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluate_model(model, val_ds)\n",
    "    \n",
    "    # Save model for deployment\n",
    "    save_model_for_deployment(model)\n",
    "    \n",
    "    # Create and test inference function\n",
    "    predict_human = create_inference_function(model)\n",
    "    \n",
    "    print(\"\\nModel training and evaluation complete!\")\n",
    "    print(\"You can now use the model for inference on new images.\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"predict_human('path/to/your/image.jpg')\")\n",
    "    \n",
    "    return predict_human\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_human = main()\n",
    "    \n",
    "    # Example of how to use the inference function (uncomment to test)\n",
    "    # test_image = \"./path/to/test/image.jpg\"\n",
    "    # if os.path.exists(test_image):\n",
    "    #     result = predict_human(test_image)\n",
    "    #     print(f\"\\nTest prediction on {test_image}:\")\n",
    "    #     print(f\"Prediction: {result['prediction']}\")\n",
    "    #     print(f\"Probability: {result['probability']:.2f}%\")\n",
    "    #     print(f\"Is human present: {result['is_human']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
